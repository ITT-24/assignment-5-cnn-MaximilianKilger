{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynput\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPooling2D, AveragePooling2D, RandomFlip, RandomContrast, RandomRotation, RandomBrightness\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.metrics import categorical_crossentropy\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "GESTURES = ['rock', 'ok', 'like', 'dislike', 'peace']\n",
    "OUTLIERS = ['two_up', 'fist', 'stop', 'one', 'three']\n",
    "SIZE = [64, 64]\n",
    "CHANNELS = 3\n",
    "PATH = \"gesture_dataset_sample\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM EXERCISE\n",
    "annotations = dict()\n",
    "\n",
    "for condition in GESTURES + OUTLIERS:\n",
    "    with open(f'{PATH}/_annotations/{condition}.json') as f:\n",
    "        annotations[condition] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM EXERCISE\n",
    "def preprocess_image(img):\n",
    "    if CHANNELS == 1:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img_resized = cv2.resize(img, SIZE)\n",
    "    return img_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa7e0bfcf404e78b6e54d91ecab26a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ca927b2f7d4ab2bf366714f3796c88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb332a22bee477e8e32ad69d73c4061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1323a61bfb39480391fbb06968df31af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf36b45e579419fbc5813df4a00aa4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662f609a03744e4cb1426a863b6c69cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2751f64dbf749768f1390363c441fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b967ce6e7e408caa6f3d63f8f61f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df81f6303f3142a696cdc001a42edace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e8bccc1fc84ed9814d284a8ec19b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#COPIED FROM EXERCISE\n",
    "\n",
    "images = [] # stores actual image data\n",
    "labels = [] # stores labels (as integer - because this is what our network needs)\n",
    "label_names = [] # maps label ints to their actual categories so we can understand predictions later\n",
    "\n",
    "# loop over all conditions\n",
    "# loop over all files in the condition's directory\n",
    "# read the image and corresponding annotation\n",
    "# crop image to the region of interest\n",
    "# preprocess image\n",
    "# store preprocessed image and label in corresponding lists\n",
    "for condition in GESTURES + OUTLIERS:\n",
    "    for filename in tqdm(os.listdir(f'{PATH}/{condition}')):\n",
    "        # extract unique ID from file name\n",
    "        UID = filename.split('.')[0]\n",
    "        img = cv2.imread(f'{PATH}/{condition}/{filename}')\n",
    "        \n",
    "        # get annotation from the dict we loaded earlier\n",
    "        try:\n",
    "            annotation = annotations[condition][UID]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        \n",
    "        # iterate over all hands annotated in the image\n",
    "        for i, bbox in enumerate(annotation['bboxes']):\n",
    "            # annotated bounding boxes are in the range from 0 to 1\n",
    "            # therefore we have to scale them to the image size\n",
    "            x1 = int(bbox[0] * img.shape[1])\n",
    "            y1 = int(bbox[1] * img.shape[0])\n",
    "            w = int(bbox[2] * img.shape[1])\n",
    "            h = int(bbox[3] * img.shape[0])\n",
    "            x2 = x1 + w\n",
    "            y2 = y1 + h\n",
    "            \n",
    "            # crop image to the bounding box and apply pre-processing\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "            preprocessed = preprocess_image(crop)\n",
    "            \n",
    "            # get the annotated hand's label\n",
    "            # if we have not seen this label yet, add it to the list of labels\n",
    "            label = annotation['labels'][i]\n",
    "            if label in OUTLIERS:\n",
    "                 label = \"no_gesture\"\n",
    "            \n",
    "            if label not in label_names:\n",
    "                    label_names.append(label)\n",
    "            \n",
    "            label_index = label_names.index(label)\n",
    "            \n",
    "            images.append(preprocessed)\n",
    "            labels.append(label_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rock', 'no_gesture', 'ok', 'like', 'dislike', 'peace']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#labels.count(label_names.index(\"no_gesture\"))\n",
    "#len(labels)\n",
    "label_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPIED FROM EXERCISE\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2586, 64, 64, 3) (647, 64, 64, 3) (2586, 6) (647, 6)\n"
     ]
    }
   ],
   "source": [
    "# COPIED FROM EXERCISE\n",
    "X_train = np.array(X_train).astype('float32')\n",
    "X_train = X_train / 255.\n",
    "\n",
    "X_test = np.array(X_test).astype('float32')\n",
    "X_test = X_test / 255.\n",
    "\n",
    "y_train_one_hot = to_categorical(y_train, num_classes=6)\n",
    "y_test_one_hot = to_categorical(y_test, num_classes=6)\n",
    "\n",
    "train_label = y_train_one_hot\n",
    "test_label = y_test_one_hot\n",
    "\n",
    "X_train = X_train.reshape(-1, SIZE[0], SIZE[0], CHANNELS)\n",
    "X_test = X_test.reshape(-1, SIZE[0], SIZE[0], CHANNELS)\n",
    "\n",
    "print(X_train.shape, X_test.shape, train_label.shape, test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2586"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "epochs = 50\n",
    "\n",
    "num_classes = len(label_names)\n",
    "activation = \"relu\"\n",
    "activation_conv = \"leaky_relu\"\n",
    "\n",
    "layer_count = 3\n",
    "num_neurons = 576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(RandomFlip('horizontal'))\n",
    "model.add(RandomBrightness(0.1))\n",
    "model.add(RandomContrast(0.1))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(9,9),activation=activation_conv,input_shape=(SIZE[0], SIZE[1], CHANNELS), padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), padding=\"same\"))\n",
    "\n",
    "#model.add(Conv2D(32, kernel_size=(5,5), activation=activation_conv, padding=\"same\"))\n",
    "#model.add(AveragePooling2D(pool_size=(3,3), padding=\"same\"))\n",
    "\n",
    "#model.add(Conv2D(32, kernel_size=(4,4), activation=activation_conv, padding=\"same\"))\n",
    "#model.add(AveragePooling2D(pool_size=(2,2), padding=\"same\"))\n",
    "\n",
    "#model.add(Conv2D(16, kernel_size=(4,4), activation=activation_conv, padding=\"same\"))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), padding=\"same\"))\n",
    "\n",
    "model.add(Dropout(0.23))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation=activation))\n",
    "#model.add(Dense(32, activation=activation))\n",
    "#model.add(Dense(16, activation=activation))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(num_classes, activation=\"softmax\"))\n",
    "model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=[\"accuracy\"])#, \"precision\", \"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define model structure\n",
    "# with keras, we can use a model's add() function to add layers to the network one by one\n",
    "model = Sequential()\n",
    "\n",
    "# data augmentation (this can also be done beforehand - but don't augment the test dataset!)\n",
    "model.add(RandomFlip('horizontal'))\n",
    "model.add(RandomContrast(0.1))\n",
    "#model.add(RandomBrightness(0.1))\n",
    "#model.add(RandomRotation(0.2))\n",
    "\n",
    "# first, we add some convolution layers followed by max pooling\n",
    "model.add(Conv2D(64, kernel_size=(9, 9), activation=activation_conv, input_shape=(SIZE[0], SIZE[1], CHANNELS), padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "model.add(Conv2D(64, (5, 5), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "# after the convolution layers, we have to flatten the data so it can be fed into fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# dropout layers can drop part of the data during each epoch - this prevents overfitting\n",
    "model.add(Dropout(0.35))\n",
    "\n",
    "# add some fully connected layers (\"Dense\")\n",
    "for i in range(layer_count - 1):\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "# for classification, the last layer has to use the softmax activation function, which gives us probabilities for each category\n",
    "model.add(Dense(num_classes, activation='softmax', input_shape=num_neurons))\n",
    "\n",
    "# specify loss function, optimizer and evaluation metrics\n",
    "# for classification, categorial crossentropy is used as a loss function\n",
    "# use the adam optimizer unless you have a good reason not to\n",
    "model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define model structure\n",
    "# with keras, we can use a model's add() function to add layers to the network one by one\n",
    "model = Sequential()\n",
    "\n",
    "# data augmentation (this can also be done beforehand - but don't augment the test dataset!)\n",
    "model.add(RandomFlip('horizontal'))\n",
    "model.add(RandomContrast(0.1))\n",
    "#model.add(RandomBrightness(0.1))\n",
    "#model.add(RandomRotation(0.2))\n",
    "\n",
    "# first, we add some convolution layers followed by max pooling\n",
    "model.add(Conv2D(64, kernel_size=(9, 9), activation=activation_conv, input_shape=(SIZE[0], SIZE[1], CHANNELS), padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(4, 4), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (5, 5), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(3, 3), padding='same'))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation=activation_conv, padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "# dropout layers can drop part of the data during each epoch - this prevents overfitting\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# after the convolution layers, we have to flatten the data so it can be fed into fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# add some fully connected layers (\"Dense\")\n",
    "for i in range(layer_count - 1):\n",
    "    model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "model.add(Dense(num_neurons, activation=activation))\n",
    "\n",
    "# for classification, the last layer has to use the softmax activation function, which gives us probabilities for each category\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# specify loss function, optimizer and evaluation metrics\n",
    "# for classification, categorial crossentropy is used as a loss function\n",
    "# use the adam optimizer unless you have a good reason not to\n",
    "model.compile(loss=categorical_crossentropy, optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 31ms/step - accuracy: 0.6115 - loss: 1.4584 - val_accuracy: 0.6352 - val_loss: 1.2423 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.6062 - loss: 1.3136 - val_accuracy: 0.6352 - val_loss: 1.2209 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6223 - loss: 1.2450 - val_accuracy: 0.6352 - val_loss: 1.2519 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6175 - loss: 1.2044 - val_accuracy: 0.6878 - val_loss: 0.9440 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 33ms/step - accuracy: 0.6597 - loss: 0.9708 - val_accuracy: 0.6924 - val_loss: 0.8641 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.6988 - loss: 0.8483 - val_accuracy: 0.7342 - val_loss: 0.7500 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7571 - loss: 0.7114 - val_accuracy: 0.7929 - val_loss: 0.5929 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7787 - loss: 0.5863 - val_accuracy: 0.8114 - val_loss: 0.5279 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.8241 - loss: 0.4940 - val_accuracy: 0.8393 - val_loss: 0.5258 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.8430 - loss: 0.4262 - val_accuracy: 0.7913 - val_loss: 0.6192 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8396 - loss: 0.4678 - val_accuracy: 0.8794 - val_loss: 0.3834 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.8897 - loss: 0.3064 - val_accuracy: 0.8377 - val_loss: 0.4850 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8863 - loss: 0.3230 - val_accuracy: 0.8856 - val_loss: 0.3956 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9054 - loss: 0.2920 - val_accuracy: 0.8748 - val_loss: 0.4065 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9206 - loss: 0.2211 - val_accuracy: 0.9057 - val_loss: 0.3900 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9428 - loss: 0.1881 - val_accuracy: 0.8686 - val_loss: 0.5014 - learning_rate: 0.0010\n"
     ]
    }
   ],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    train_label,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    validation_data=(X_test, test_label),\n",
    "    callbacks=[reduce_lr, stop_early]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_20\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_20\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">15,616</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,232</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_54 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">166,464</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">332,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">332,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,462</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_52 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m15,616\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_52 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_53 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │        \u001b[38;5;34m51,232\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_53 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_54 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │         \u001b[38;5;34m9,248\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_54 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m32\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_20 (\u001b[38;5;33mFlatten\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m288\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_73 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │       \u001b[38;5;34m166,464\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_74 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │       \u001b[38;5;34m332,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_75 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m576\u001b[0m)            │       \u001b[38;5;34m332,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_76 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m3,462\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,732,180</span> (10.42 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,732,180\u001b[0m (10.42 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">910,726</span> (3.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m910,726\u001b[0m (3.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,821,454</span> (6.95 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,821,454\u001b[0m (6.95 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('gesture_recognition_for_media_control_1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
